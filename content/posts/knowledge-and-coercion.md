+++
title = "Draft: Knowledge and Coercion"
date = "2020-12-17"
description = "Does the growth of knowledge suggest that we will have more or less coercion in the future?"
tags = ["economics", "violence"]
draft = "true"

+++

We only need coercion when we lack the knowledge of how to get something done with cooperation, and so as our knowledge grows, we will need less coercion. However, coercion slows the growth of knowledge, thus keeping itself necessary.

---

A very important and under-theorized question - will the need for coercion increase or decrease as we gain more knowledge?

The people who have thought about this question anticipate a much greater need for coercion in the future as we develop new technologies. Nick Bostrom argues in [The Vulnerable World Hypothesis](https://www.nickbostrom.com/papers/vulnerable.pdf) that

> [T]here is some level of technology at which civilization almost certainly gets destroyed unless quite extraordinary and historically unprecedented degrees of preventive policing and/or global governance are implemented.

If this is true, we have a bleak future ahead of us. Eventually, we will have to choose between either letting any college biology student manufacture the next great pandemic, or creating a high-tech panopticon where the state sees everything you discover or build. According to this view, as we gain more knowledge, the need for coercion increases.

I want to defend the opposite view - that the need for coercion will shrink instead of grow as we learn more about the world and about systems of human cooperation.

# Solving Problems Without Coercion

> Optimism ... is the theory that all failures—all evils—are due to insufficient knowledge ... Problems are inevitable, because our knowledge will always be infinitely far from complete. Some problems are hard, but it is a mistake to confuse hard problems with problems unlikely to be solved. Problems are soluble, and each particular evil is a problem that can be solved. An optimistic civilization is open and not afraid to innovate, and is based on traditions of criticism. Its institutions keep improving, and the most important knowledge that they embody is knowledge of how to detect and eliminate errors.
>
> David Deutsch, [The Beginning of Infinity](https://www.amazon.com/Beginning-Infinity-Explanations-Transform-World/dp/0143121359/)

Many assume that we need coercion to solve certain problems. We not only have to regulate powerful new technologies, as Bostrom advocates, but we also cannot have a functioning society if people are not compelled to do certain things. We must force people to contribute taxes and remove people's freedom when they break laws.

I first suspected that this might not be true when I read about Alex Tabarrok's idea to solve the public goods problem. We have long thought that when society has to invest in goods which benefit people who don't contribute as well as those who do, it is necessary to force people to pony up. Since the Navy defends all the citizens from amphibious attacks, and not just those who would voluntarily chip in, it is necessary to make people pay for the common defense.

Tabarrok solves the public goods problem with dominant assurance contracts. It's like Kickstarter for public goods, except the entrepreneur has to pay back interest to the people who contributed if not enough money is raised. This makes contributing to public goods the dominant strategy. If the contract raises enough money, you get a valuable public good, and if it doesn't, you get your money back with interest - either way, you benefit by contributing.

The point is, here is this ancient and fundamental problem about human cooperation which we assumed we needed coercion to solve. You cannot travel faster than light, you cannot decrease entropy, and you cannot get public goods without forcing people to contribute. For thousands of years, states have been forcing people to pay taxes so that infrastructure can be built and armies can be raised. And then Tabarrok comes along and creates knowledge about how incentives can be aligned so that people benefit from contributing to public goods.

What other perennial problems (which we currently use coercion to solve) can be solved without using coercion - that is, if we gain the necessary knowledge? By knowledge, I do not just mean new technologies, although they are necessary as well. I also mean the knowledge about how to persuade people and how to create systems that productively use incentives and information.

**Given sufficient knowledge, all problems are solvable without coercion. These non-coercive solutions are better because coercion stalls the growth of further knowledge, and by doing so keeps itself necessary.**

This principle is most evident when government creates a coercive solution to a problem which crowds out the opportunities for entrepreneurs solve it. For example, the government may say that it has to determine how a contract dispute should be settled or which meats are

# Knowledge and destruction

So far, I have made the case that the growth of knowledge will help us solve many problems noncoercively. But what about the cases in which a piece of knowledge presents such a threat to civilization that coercion must be used to prevent people from discovering or using that knowledge? Bostrom explains that creating new technologies is like pulling balls out of a bag:

> What we haven’t extracted, so far, is a black ball: a technology that invariably or by default destroys the civilization that invents it. The reason is not that we have been particularly careful or wise in our technology policy. We have just been lucky.

Bostrom asks us to imagine a counterfactual in which creating nuclear bombs was so simple that any individual could make one easily. He argues that there is no reason to

> That [easy nukes present a civilizational vulnerability] almost follows from the law of large numbers combined with the plausible assumption that for any randomly selected person there is some small but appreciable chance that they would be motivated to trigger this kind of destruction – whether out of ideological hatred, nihilistic destructiveness, revenge for perceived injustices, as part of some extortion plot, or because of delusions or mental illness, or perhaps even just to see what would happen.

**The argument that there will be a lot more destruction as it becomes cheaper to kill a lot of people suffers from one major flaw - it's already really cheap to kill a lot of people.** It [costs far less](https://www.gwern.net/docs/www/www.nato.int/c3fe55ee2c40a312f1cbd63cb6e7308ecf6e20bf.html) to build a suicide bomb than it does to buy a smartphone. If you can't even spare that much, just steal a truck GTA style and drive into a crowd. Or if have a strategic goal, you and a few dozen friends could destroy a major multinational corporation with [this scheme](https://www.gwern.net/Terrorism-is-not-Effective#on-the-absence-of-true-fanatics) Gwern came up with.

And yet the worldwide [share of deaths from terrorism](https://ourworldindata.org/terrorism#what-share-of-deaths-are-from-terrorism) in 2017 (the last year on record) was only 0.06%. Furthermore, these deaths are overwhelmingly concentrated in the parts of the world that are undergoing civil wars - in other words, the destruction is happening in the places where there is the least left to destroy.

If Bostrom is right, the law of large numbers should imply that there are tons of hateful and nihilistic people who are taking advantage of the many existing opportunities for destruction. Our cities should be unlivable due to continuous terrorist attacks. To the extent that these people can work themselves into positions of power, they would have have nudged our nations towards genocides and nuclear wars. But thankfully, the levels of violence in the world are far lower than this kind of analysis would suggest.

This tells us two things. First, we should be more optimistic about whether people will use cheap new technologies to kill a lot of people. Second, it is a nonsensical argument to say that we have to use coercion to arrest the growth of knowledge on the off-chance that we discover new ways to easily kill a lot of people, because we already have lots of easy methods to inflict mass casualty. And there is no way to undiscover bombmaking or waterpoisoning.

That knowledge is often dangerous is a problem - but is cannot be solved by preventing the further growth of knowledge when dangerous and insuppressible knowledge already exists. The only real solution is to create more knowledge about how to make existing knowledge less dangerous, and to do so as quickly as possible.
